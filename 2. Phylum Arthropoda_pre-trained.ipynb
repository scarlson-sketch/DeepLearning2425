{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **<h3 align=\"center\"> Deep Learning - Project </h3>**\n",
    "# **<h3 align=\"center\"> Phylum Arthropoda - Steven</h3>**\n",
    "**Group 4 members:**<br>\n",
    "Alexandra Pinto - 20211599@novaims.unl.pt - 20211599<br>\n",
    "Steven Carlson - 20240554@novaims.unl.pt - 20240554<br>\n",
    "Sven Goerdes - 20240503@novaims.unl.pt - 20240503<br>\n",
    "Tim Straub - 20240505@novaims.unl.pt - 20240505<br>\n",
    "Zofia Wojcik  - 20240654@novaims.unl.pt - 20240654<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "* [1. Introduction](#intro)\n",
    "* [2. Setup](#setup)\n",
    "* [3. Data Loading](#dataloading)\n",
    "* [4. Image Preprocessing](#imagepreprocessing)\n",
    "* [5. Neural Networks Models](#nnmodels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction <a class=\"anchor\" id=\"intro\"></a>\n",
    "\n",
    "In this second notebook, we will preprocess images from the **Arthropoda** phylum and develop a deep learning model to accurately classify them at the family level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# 2. Setup <a class=\"anchor\" id=\"setup\"></a>\n",
    "In this section, we will import the necessary libraries that will be used throughout the notebook. These libraries will help with data handling and image processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import zipfile\n",
    "import seaborn as sns\n",
    "\n",
    "# Libraries for image processing\n",
    "from glob import glob\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries from Keras / TensorFlow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from keras.utils import image_dataset_from_directory\n",
    "from keras import optimizers\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "#Import pre-trained models\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input as resnet_preprocess\n",
    "\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input as mobilenet_preprocess\n",
    "\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input as efficientnet_preprocess\n",
    "\n",
    "from tensorflow.keras.applications import DenseNet121\n",
    "from tensorflow.keras.applications.densenet import preprocess_input as densenet_preprocess\n",
    "\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras.applications.inception_v3 import preprocess_input as inception_preprocess\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Loading <a class=\"anchor\" id=\"dataloading\"></a>\n",
    "\n",
    "Let's open the train and test for Arthropoda Phylum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eol_content_id</th>\n",
       "      <th>eol_page_id</th>\n",
       "      <th>kingdom</th>\n",
       "      <th>phylum</th>\n",
       "      <th>family</th>\n",
       "      <th>file_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28260809</td>\n",
       "      <td>1065329</td>\n",
       "      <td>animalia</td>\n",
       "      <td>arthropoda</td>\n",
       "      <td>apidae</td>\n",
       "      <td>arthropoda_apidae/28260809_1065329_eol-full-si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29945328</td>\n",
       "      <td>1077217</td>\n",
       "      <td>animalia</td>\n",
       "      <td>arthropoda</td>\n",
       "      <td>pseudophasmatidae</td>\n",
       "      <td>arthropoda_pseudophasmatidae/29945328_1077217_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14644212</td>\n",
       "      <td>463474</td>\n",
       "      <td>animalia</td>\n",
       "      <td>arthropoda</td>\n",
       "      <td>formicidae</td>\n",
       "      <td>arthropoda_formicidae/14644212_463474_eol-full...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   eol_content_id  eol_page_id   kingdom      phylum             family  \\\n",
       "0        28260809      1065329  animalia  arthropoda             apidae   \n",
       "1        29945328      1077217  animalia  arthropoda  pseudophasmatidae   \n",
       "2        14644212       463474  animalia  arthropoda         formicidae   \n",
       "\n",
       "                                           file_path  \n",
       "0  arthropoda_apidae/28260809_1065329_eol-full-si...  \n",
       "1  arthropoda_pseudophasmatidae/29945328_1077217_...  \n",
       "2  arthropoda_formicidae/14644212_463474_eol-full...  "
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the DataFrame from the CSV file\n",
    "arthropoda_train = pd.read_csv(\"/home/sacar/DeepLearning2425/train_test_splits/arthropoda_train.csv\")\n",
    "arthropoda_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eol_content_id</th>\n",
       "      <th>eol_page_id</th>\n",
       "      <th>kingdom</th>\n",
       "      <th>phylum</th>\n",
       "      <th>family</th>\n",
       "      <th>file_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28408206</td>\n",
       "      <td>1065346</td>\n",
       "      <td>animalia</td>\n",
       "      <td>arthropoda</td>\n",
       "      <td>apidae</td>\n",
       "      <td>arthropoda_apidae/28408206_1065346_eol-full-si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28253620</td>\n",
       "      <td>1065348</td>\n",
       "      <td>animalia</td>\n",
       "      <td>arthropoda</td>\n",
       "      <td>apidae</td>\n",
       "      <td>arthropoda_apidae/28253620_1065348_eol-full-si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21847584</td>\n",
       "      <td>1065348</td>\n",
       "      <td>animalia</td>\n",
       "      <td>arthropoda</td>\n",
       "      <td>apidae</td>\n",
       "      <td>arthropoda_apidae/21847584_1065348_eol-full-si...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   eol_content_id  eol_page_id   kingdom      phylum  family  \\\n",
       "0        28408206      1065346  animalia  arthropoda  apidae   \n",
       "1        28253620      1065348  animalia  arthropoda  apidae   \n",
       "2        21847584      1065348  animalia  arthropoda  apidae   \n",
       "\n",
       "                                           file_path  \n",
       "0  arthropoda_apidae/28408206_1065346_eol-full-si...  \n",
       "1  arthropoda_apidae/28253620_1065348_eol-full-si...  \n",
       "2  arthropoda_apidae/21847584_1065348_eol-full-si...  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the DataFrame from the CSV file\n",
    "arthropoda_test = pd.read_csv(\"/home/sacar/DeepLearning2425/train_test_splits/arthropoda_test.csv\")\n",
    "arthropoda_test.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Image Preprocessing <a class=\"anchor\" id=\"imagepreprocessing\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define some stuff\n",
    "num_classes = arthropoda_train['family'].nunique() #number of classes = number of families\n",
    "batch_size = 64\n",
    "input_shape = (224, 224, 3)\n",
    "image_size = (224, 224)\n",
    "value_range = (0.0, 1.0)\n",
    "num_classes = 17  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define preprocess and augmentation functions\n",
    "\n",
    "#Function to preprocess the images\n",
    "def process_image(file_path, label):\n",
    "    image = tf.io.read_file(file_path) # Read the image file\n",
    "    image = tf.image.decode_jpeg(image, channels=3) # Decode the JPEG image\n",
    "    image = tf.image.resize(image, image_size) # Resize the image to the target size\n",
    "    \n",
    "    #CHANGE THIS LINE DEPENDING ON WHICH PRE-TRAINED MODEL IS BEING USED\n",
    "    \n",
    "    #image = resnet_preprocess(image)  # Apply ResNet50 preprocessing\n",
    "    #image = mobilenet_preprocess(image)  # Apply MobileNetV2 preprocessing\n",
    "    #image = efficientnet_preprocess(image)  # Apply EfficientNetB0 preprocessing\n",
    "    image = densenet_preprocess(image)  # Apply DenseNet121 preprocessing\n",
    "    #image = inception_preprocess(image)  # Apply InceptionV3 preprocessing\n",
    "    #image = tf.cast(image, tf.float32) / 255.0  # Apply ConvNeXt\n",
    "    return image, label\n",
    "\n",
    "\n",
    "#Function to augment the images\n",
    "def augment_image(image, label):\n",
    "\n",
    "    #Randomly change brightness\n",
    "    image = tf.image.random_brightness(image, max_delta=0.2)\n",
    "\n",
    "    #Apply geometric augmentations\n",
    "    image = geometric_augmentation_layers(image, training=True) # Apply geometric augmentations\n",
    "    image = tf.clip_by_value(image, 0.0, 1.0)\n",
    "    \n",
    "    return image, label\n",
    "\n",
    "\n",
    "# Geometric augmentations\n",
    "geometric_augmentation_layers = tf.keras.Sequential(\n",
    "    [\n",
    "        # Randomly flip horizontally\n",
    "        tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "\n",
    "        # Randomly rotate\n",
    "        tf.keras.layers.RandomRotation(factor=0.12),\n",
    "\n",
    "        # Random zoom\n",
    "        tf.keras.layers.RandomZoom(height_factor=(-0.35, 0.35), # Corresponds to [0.8, 1.2] of original height\n",
    "                                   width_factor=(-0.35, 0.35)), # Corresponds to [0.8, 1.2] of original width\n",
    "\n",
    "        # Random shift\n",
    "        tf.keras.layers.RandomTranslation(height_factor=0.20,\n",
    "                                          width_factor=0.20),\n",
    "\n",
    "        # Contrast\n",
    "        tf.keras.layers.RandomContrast(factor=0.25),\n",
    "\n",
    "    ],\n",
    "    name=\"geometric_augmentations\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/sacar/DeepLearning2425/rare_species/arthropoda_apidae/28260809_1065329_eol-full-size-copy.jpg', '/home/sacar/DeepLearning2425/rare_species/arthropoda_pseudophasmatidae/29945328_1077217_eol-full-size-copy.jpg', '/home/sacar/DeepLearning2425/rare_species/arthropoda_formicidae/14644212_463474_eol-full-size-copy.jpg', '/home/sacar/DeepLearning2425/rare_species/arthropoda_pseudophasmatidae/29945335_1077217_eol-full-size-copy.jpg', '/home/sacar/DeepLearning2425/rare_species/arthropoda_papilionidae/21035374_130548_eol-full-size-copy.jpg']\n",
      "[0, 13, 5, 13, 10]\n",
      "['/home/sacar/DeepLearning2425/rare_species/arthropoda_apidae/28408206_1065346_eol-full-size-copy.jpg', '/home/sacar/DeepLearning2425/rare_species/arthropoda_apidae/28253620_1065348_eol-full-size-copy.jpg', '/home/sacar/DeepLearning2425/rare_species/arthropoda_apidae/21847584_1065348_eol-full-size-copy.jpg', '/home/sacar/DeepLearning2425/rare_species/arthropoda_formicidae/14681521_403723_eol-full-size-copy.jpg', '/home/sacar/DeepLearning2425/rare_species/arthropoda_palinuridae/30050875_46516728_eol-full-size-copy.jpg']\n",
      "[0, 0, 0, 5, 9]\n"
     ]
    }
   ],
   "source": [
    "root_dir = \"/home/sacar/DeepLearning2425/rare_species\"\n",
    "\n",
    "#Get the file paths and labels for the training and test set\n",
    "arthropoda_train['full_path'] = arthropoda_train['file_path'].apply(lambda x: os.path.normpath(os.path.join(root_dir, x)))\n",
    "arthropoda_test['full_path'] = arthropoda_test['file_path'].apply(lambda x: os.path.normpath(os.path.join(root_dir, x)))\n",
    "\n",
    "file_paths_train = arthropoda_train['full_path'].tolist()\n",
    "labels_train = arthropoda_train['family'].tolist()\n",
    "\n",
    "file_paths_test = arthropoda_test['full_path'].tolist()\n",
    "labels_test = arthropoda_test['family'].tolist()\n",
    "\n",
    "#Map the labels to integers\n",
    "label_names = sorted(set(labels_train)) # Get the unique labels\n",
    "label_to_index = {name: i for i, name in enumerate(label_names)} # Create a mapping from labels to integers\n",
    "labels_train = [label_to_index[label] for label in labels_train]\n",
    "labels_test = [label_to_index[label] for label in labels_test]\n",
    "\n",
    "print(file_paths_train[:5])\n",
    "print(labels_train[:5])\n",
    "print(file_paths_test[:5])\n",
    "print(labels_test[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the tensorflow datasets\n",
    "\n",
    "#Load data\n",
    "data = tf.data.Dataset.from_tensor_slices((file_paths_train, labels_train))\n",
    "data = data.shuffle(buffer_size=len(file_paths_train), reshuffle_each_iteration=False, seed=42) # Shuffle the dataset\n",
    "\n",
    "#Create train/val\n",
    "train_size = int(0.8 * len(file_paths_train)) #80% for training, 20% for validation\n",
    "train = data.take(train_size) # Take the first 80% for training\n",
    "val = data.skip(train_size) # Skip the first 80% for validation\n",
    "\n",
    "#Training preprocess pipeline\n",
    "train = train.map(process_image, num_parallel_calls=tf.data.AUTOTUNE) # Map the function to the dataset\n",
    "train = train.cache() # Cache the dataset for faster access\n",
    "train = train.map(augment_image, num_parallel_calls=tf.data.AUTOTUNE) # Map the function to the dataset\n",
    "train = train.shuffle(buffer_size=len(file_paths_train), reshuffle_each_iteration=True, seed=42).batch(8).prefetch(buffer_size=tf.data.AUTOTUNE) #shuffle and batch\n",
    "\n",
    "#Validation preprocess pipeline\n",
    "val = val.map(process_image, num_parallel_calls=tf.data.AUTOTUNE) # Map the function to the dataset\n",
    "val = val.cache() # Cache the dataset for faster access\n",
    "val = val.batch(8).prefetch(buffer_size=tf.data.AUTOTUNE) #batch\n",
    "\n",
    "\n",
    "#Test preprocess pipeline\n",
    "test = tf.data.Dataset.from_tensor_slices((file_paths_test, labels_test))\n",
    "test = test.map(process_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "test = test.cache().batch(8).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: (8, 224, 224, 3)\n",
      "Label: [0 8 5 5 2 8 0 5]\n",
      "Image shape: (8, 224, 224, 3)\n",
      "Label: [4 0 5 5 5 6 0 1]\n",
      "Image shape: (8, 224, 224, 3)\n",
      "Label: [ 3  5  3 11  6 10  1  3]\n",
      "Image shape: (8, 224, 224, 3)\n",
      "Label: [ 0  0  0  5  9  0 16  5]\n",
      "Image shape: (8, 224, 224, 3)\n",
      "Label: [ 5  5  0 16  6  0 11  0]\n",
      "Image shape: (8, 224, 224, 3)\n",
      "Label: [ 0  5 13  5  5  8  5  0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-10 11:40:58.789859: W tensorflow/core/kernels/data/cache_dataset_ops.cc:916] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "for image, label in train.take(3):\n",
    "    print(\"Image shape:\", image.numpy().shape)\n",
    "    print(\"Label:\", label.numpy())\n",
    "\n",
    "for image, label in test.take(3):\n",
    "    print(\"Image shape:\", image.numpy().shape)\n",
    "    print(\"Label:\", label.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Neural Network Models <a class=\"anchor\" id=\"nnmodels\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Physical devices cannot be modified after being initialized\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ GPU(s) detected: ['/physical_device:GPU:0']\n"
     ]
    }
   ],
   "source": [
    "# Print GPU devices detected\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f\"✅ GPU(s) detected: {[gpu.name for gpu in gpus]}\")\n",
    "else:\n",
    "    print(\"❌ No GPU detected by TensorFlow.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the model to run\n",
    "is_resnet = 0\n",
    "is_mobilenet = 0\n",
    "is_efficientnet = 0\n",
    "is_densenet = 1\n",
    "is_inception = 0\n",
    "is_convnext = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_resnet == 1:\n",
    "    # 1. Load the ResNet50 base model (without top classifier)\n",
    "    base_model = ResNet50(\n",
    "        weights='imagenet',          # Use ImageNet pretrained weights\n",
    "        include_top=False,           # Exclude the default final dense layer\n",
    "        input_shape=(224, 224, 3)    # Adjust to match your input images\n",
    "    )\n",
    "\n",
    "    # 2. Freeze the base model so we only train the classifier head\n",
    "    base_model.trainable = False\n",
    "\n",
    "    # 3. Build custom classification head\n",
    "    x = base_model.output\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    output = layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "\n",
    "    # 4. Create the full model\n",
    "    model = models.Model(inputs=base_model.input, outputs=output)\n",
    "\n",
    "    # 5. Compile the model\n",
    "    model.compile(\n",
    "        optimizer=optimizers.Adam(learning_rate=1e-5),\n",
    "        loss='sparse_categorical_crossentropy',  # ✅ Use sparse version for integer labels\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "\n",
    "    # 6. Train the model\n",
    "    history = model.fit(\n",
    "        train,\n",
    "        epochs=100,\n",
    "        validation_data=val,\n",
    "        callbacks=[EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # 7. Save the model\n",
    "    model.save(\"resnet50_rare_species_model.h5\")\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "if is_mobilenet == 1:\n",
    "    # Load base model without top layer\n",
    "    base_model = MobileNetV2(\n",
    "        input_shape=(224, 224, 3),  # Match your resized image shape\n",
    "        include_top=False,         # Don't include the original classifier\n",
    "        weights='imagenet'         # Use ImageNet-pretrained weights\n",
    "    )\n",
    "\n",
    "    # Freeze base model so we only train the new layers for now\n",
    "    base_model.trainable = False\n",
    "\n",
    "    # Build custom model\n",
    "    model = models.Sequential([\n",
    "        base_model,                                # Feature extractor\n",
    "        layers.GlobalAveragePooling2D(),           # Pool over spatial dimensions\n",
    "        layers.Dropout(0.3),                       # Regularization layer\n",
    "        layers.Dense(128, activation='relu'),      # Fully connected layer\n",
    "        layers.Dense(num_classes, activation='softmax')      # Output layer (5 mollusk families)\n",
    "    ])\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer=optimizers.Adam(learning_rate=1e-4),\n",
    "        loss='sparse_categorical_crossentropy',  # Use sparse version for integer labels\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    # Summary for your viewing pleasure\n",
    "    model.summary()\n",
    "\n",
    "    history = model.fit(\n",
    "        train,\n",
    "        validation_data=val,\n",
    "        epochs=100,           # Start small, increase if needed\n",
    "        verbose=1\n",
    "    )\n",
    "    # Execution time: 9m 58.2s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n",
      "\u001b[1m16705208/16705208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ efficientnetb0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,049,571</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d_11     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">163,968</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,193</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ efficientnetb0 (\u001b[38;5;33mFunctional\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1280\u001b[0m)     │     \u001b[38;5;34m4,049,571\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d_11     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_11 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_22 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m163,968\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_23 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m17\u001b[0m)             │         \u001b[38;5;34m2,193\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,215,732</span> (16.08 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,215,732\u001b[0m (16.08 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">166,161</span> (649.07 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m166,161\u001b[0m (649.07 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,049,571</span> (15.45 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m4,049,571\u001b[0m (15.45 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 436ms/step - accuracy: 0.2578 - loss: 2.6305 - val_accuracy: 0.0338 - val_loss: 3.1140\n",
      "Epoch 2/100\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 28ms/step - accuracy: 0.3029 - loss: 2.4749 - val_accuracy: 0.0338 - val_loss: 3.1588\n",
      "Epoch 3/100\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 27ms/step - accuracy: 0.2882 - loss: 2.4413 - val_accuracy: 0.0338 - val_loss: 3.1755\n",
      "Epoch 4/100\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 27ms/step - accuracy: 0.3405 - loss: 2.4151 - val_accuracy: 0.0338 - val_loss: 3.1772\n",
      "Epoch 5/100\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 27ms/step - accuracy: 0.3005 - loss: 2.4014 - val_accuracy: 0.0338 - val_loss: 3.1964\n",
      "Epoch 6/100\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 27ms/step - accuracy: 0.3045 - loss: 2.4694 - val_accuracy: 0.0338 - val_loss: 3.2190\n",
      "Epoch 7/100\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 26ms/step - accuracy: 0.2879 - loss: 2.4912 - val_accuracy: 0.0338 - val_loss: 3.2346\n",
      "Epoch 8/100\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 45ms/step - accuracy: 0.3314 - loss: 2.4002 - val_accuracy: 0.0338 - val_loss: 3.2155\n",
      "Epoch 9/100\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 27ms/step - accuracy: 0.2971 - loss: 2.4718 - val_accuracy: 0.0270 - val_loss: 3.2556\n",
      "Epoch 10/100\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 26ms/step - accuracy: 0.3096 - loss: 2.4199 - val_accuracy: 0.0270 - val_loss: 3.2491\n",
      "Epoch 11/100\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 27ms/step - accuracy: 0.3178 - loss: 2.3957 - val_accuracy: 0.0270 - val_loss: 3.2472\n",
      "Epoch 12/100\n"
     ]
    }
   ],
   "source": [
    "if is_efficientnet == 1:\n",
    "    # Load EfficientNetB0 as base model\n",
    "    base_model = EfficientNetB0(\n",
    "        input_shape=(224, 224, 3),\n",
    "        include_top=False,\n",
    "        weights='imagenet'\n",
    "    )\n",
    "\n",
    "    # Freeze base model\n",
    "    base_model.trainable = False\n",
    "\n",
    "    # Build classifier on top\n",
    "    model = models.Sequential([\n",
    "        base_model,                                # Feature extractor\n",
    "        layers.GlobalAveragePooling2D(),           # Pool features\n",
    "        layers.Dropout(0.3),                       # Regularization\n",
    "        layers.Dense(128, activation='relu'),      # Fully connected layer\n",
    "        layers.Dense(num_classes, activation='softmax')  # Output layer\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=optimizers.Adam(learning_rate=1e-4),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    # Print summary\n",
    "    model.summary()\n",
    "\n",
    "    # Train it\n",
    "    history = model.fit(\n",
    "        train,\n",
    "        validation_data=val,\n",
    "        epochs=100,         # Adjust as needed\n",
    "        verbose=1\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_densenet == 1:\n",
    "    base_model = DenseNet121(\n",
    "        input_shape=(224, 224, 3),\n",
    "        include_top=False,\n",
    "        weights='imagenet'\n",
    "    )\n",
    "\n",
    "    base_model.trainable = False\n",
    "\n",
    "    model = models.Sequential([\n",
    "        base_model,\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizers.Adam(learning_rate=1e-4),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    history = model.fit(\n",
    "        train,\n",
    "        validation_data=val,\n",
    "        epochs=100,\n",
    "        verbose=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_inception == 1:\n",
    "    base_model = InceptionV3(\n",
    "        input_shape=(224, 224, 3),\n",
    "        include_top=False,\n",
    "        weights='imagenet'\n",
    "    )\n",
    "\n",
    "    base_model.trainable = False\n",
    "\n",
    "    model = models.Sequential([\n",
    "        base_model,\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizers.Adam(learning_rate=1e-4),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    history = model.fit(\n",
    "        train,\n",
    "        validation_data=val,\n",
    "        epochs=100,\n",
    "        verbose=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_convnext == 1:\n",
    "    base_model = tf.keras.Sequential([\n",
    "        hub.KerasLayer(\n",
    "            \"https://tfhub.dev/sayakpaul/convnext_tiny_1k_224/1\",\n",
    "            trainable=False,\n",
    "            input_shape=(224, 224, 3)\n",
    "        )\n",
    "    ])\n",
    "\n",
    "    model = models.Sequential([\n",
    "        base_model,\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizers.Adam(learning_rate=1e-4),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    history = model.fit(\n",
    "        train,\n",
    "        validation_data=val,\n",
    "        epochs=100,\n",
    "        verbose=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_wsl_env)",
   "language": "python",
   "name": "tf_wsl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
