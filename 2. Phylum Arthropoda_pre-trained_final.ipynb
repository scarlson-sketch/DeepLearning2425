{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **<h3 align=\"center\"> Deep Learning - Project </h3>**\n",
    "# **<h3 align=\"center\"> Phylum Arthropoda - Steven</h3>**\n",
    "**Group 4 members:**<br>\n",
    "Alexandra Pinto - 20211599@novaims.unl.pt - 20211599<br>\n",
    "Steven Carlson - 20240554@novaims.unl.pt - 20240554<br>\n",
    "Sven Goerdes - 20240503@novaims.unl.pt - 20240503<br>\n",
    "Tim Straub - 20240505@novaims.unl.pt - 20240505<br>\n",
    "Zofia Wojcik  - 20240654@novaims.unl.pt - 20240654<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "* [1. Introduction](#intro)\n",
    "* [2. Setup](#setup)\n",
    "* [3. Data Loading](#dataloading)\n",
    "* [4. Image Preprocessing](#imagepreprocessing)\n",
    "* [5. Neural Networks Models](#nnmodels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction <a class=\"anchor\" id=\"intro\"></a>\n",
    "\n",
    "In this second notebook, we will preprocess images from the **Arthropoda** phylum and develop a deep learning model to accurately classify them at the family level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# 2. Setup <a class=\"anchor\" id=\"setup\"></a>\n",
    "In this section, we will import the necessary libraries that will be used throughout the notebook. These libraries will help with data handling and image processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import zipfile\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "# Libraries for image processing\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries from Keras / TensorFlow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from keras.utils import image_dataset_from_directory\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "import gc\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "\n",
    "#import tensorflow_hub as hub\n",
    "\n",
    "#Import pre-trained models\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input as resnet_preprocess\n",
    "\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input as mobilenet_preprocess\n",
    "\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input as efficientnet_preprocess\n",
    "\n",
    "from tensorflow.keras.applications import DenseNet121\n",
    "from tensorflow.keras.applications.densenet import preprocess_input as densenet_preprocess\n",
    "\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras.applications.inception_v3 import preprocess_input as inception_preprocess\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.run_functions_eagerly(False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Loading <a class=\"anchor\" id=\"dataloading\"></a>\n",
    "\n",
    "Let's open the train and test for Arthropoda Phylum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eol_content_id</th>\n",
       "      <th>eol_page_id</th>\n",
       "      <th>kingdom</th>\n",
       "      <th>phylum</th>\n",
       "      <th>family</th>\n",
       "      <th>file_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28260809</td>\n",
       "      <td>1065329</td>\n",
       "      <td>animalia</td>\n",
       "      <td>arthropoda</td>\n",
       "      <td>apidae</td>\n",
       "      <td>arthropoda_apidae/28260809_1065329_eol-full-si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29945328</td>\n",
       "      <td>1077217</td>\n",
       "      <td>animalia</td>\n",
       "      <td>arthropoda</td>\n",
       "      <td>pseudophasmatidae</td>\n",
       "      <td>arthropoda_pseudophasmatidae/29945328_1077217_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14644212</td>\n",
       "      <td>463474</td>\n",
       "      <td>animalia</td>\n",
       "      <td>arthropoda</td>\n",
       "      <td>formicidae</td>\n",
       "      <td>arthropoda_formicidae/14644212_463474_eol-full...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   eol_content_id  eol_page_id   kingdom      phylum             family  \\\n",
       "0        28260809      1065329  animalia  arthropoda             apidae   \n",
       "1        29945328      1077217  animalia  arthropoda  pseudophasmatidae   \n",
       "2        14644212       463474  animalia  arthropoda         formicidae   \n",
       "\n",
       "                                           file_path  \n",
       "0  arthropoda_apidae/28260809_1065329_eol-full-si...  \n",
       "1  arthropoda_pseudophasmatidae/29945328_1077217_...  \n",
       "2  arthropoda_formicidae/14644212_463474_eol-full...  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the DataFrame from the CSV file\n",
    "arthropoda_train = pd.read_csv(\"/home/sacar/DeepLearning2425/train_test_splits/arthropoda_train.csv\")\n",
    "arthropoda_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eol_content_id</th>\n",
       "      <th>eol_page_id</th>\n",
       "      <th>kingdom</th>\n",
       "      <th>phylum</th>\n",
       "      <th>family</th>\n",
       "      <th>file_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28408206</td>\n",
       "      <td>1065346</td>\n",
       "      <td>animalia</td>\n",
       "      <td>arthropoda</td>\n",
       "      <td>apidae</td>\n",
       "      <td>arthropoda_apidae/28408206_1065346_eol-full-si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28253620</td>\n",
       "      <td>1065348</td>\n",
       "      <td>animalia</td>\n",
       "      <td>arthropoda</td>\n",
       "      <td>apidae</td>\n",
       "      <td>arthropoda_apidae/28253620_1065348_eol-full-si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21847584</td>\n",
       "      <td>1065348</td>\n",
       "      <td>animalia</td>\n",
       "      <td>arthropoda</td>\n",
       "      <td>apidae</td>\n",
       "      <td>arthropoda_apidae/21847584_1065348_eol-full-si...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   eol_content_id  eol_page_id   kingdom      phylum  family  \\\n",
       "0        28408206      1065346  animalia  arthropoda  apidae   \n",
       "1        28253620      1065348  animalia  arthropoda  apidae   \n",
       "2        21847584      1065348  animalia  arthropoda  apidae   \n",
       "\n",
       "                                           file_path  \n",
       "0  arthropoda_apidae/28408206_1065346_eol-full-si...  \n",
       "1  arthropoda_apidae/28253620_1065348_eol-full-si...  \n",
       "2  arthropoda_apidae/21847584_1065348_eol-full-si...  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the DataFrame from the CSV file\n",
    "arthropoda_test = pd.read_csv(\"/home/sacar/DeepLearning2425/train_test_splits/arthropoda_test.csv\")\n",
    "arthropoda_test.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Image Preprocessing <a class=\"anchor\" id=\"imagepreprocessing\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map model names to their preprocessors and classes\n",
    "model_map = {\n",
    "    'resnet50':      (ResNet50,      resnet_preprocess),\n",
    "    'MobileNetV2':   (MobileNetV2,   mobilenet_preprocess),\n",
    "    'efficientnetb0':(EfficientNetB0, efficientnet_preprocess),\n",
    "    'densenet121':   (DenseNet121,   densenet_preprocess),\n",
    "    'inceptionv3':   (InceptionV3,   inception_preprocess),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define preprocess and augmentation functions\n",
    "\n",
    "#Function to preprocess the images\n",
    "def process_image(file_path, label, preprocess_fn):\n",
    "    image = tf.io.read_file(file_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, image_size)\n",
    "    image = process_image_with_clahe(file_path)  # Apply CLAHE\n",
    "\n",
    "    if preprocess_fn:\n",
    "        image = preprocess_fn(image)\n",
    "    else:\n",
    "        # default normalization\n",
    "        image = tf.cast(image, tf.float32) / 255.0\n",
    "\n",
    "    return image, label\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Function to augment the images\n",
    "def augment_image(image, label):\n",
    "\n",
    "    #Randomly change brightness\n",
    "    image = tf.image.random_brightness(image, max_delta=0.2)\n",
    "\n",
    "    #Apply geometric augmentations\n",
    "    image = geometric_augmentation_layers(image, training=True) # Apply geometric augmentations\n",
    "    image = tf.clip_by_value(image, 0.0, 1.0)\n",
    "    \n",
    "    return image, label\n",
    "\n",
    "\n",
    "# Geometric augmentations\n",
    "geometric_augmentation_layers = tf.keras.Sequential(\n",
    "    [\n",
    "        # Randomly flip horizontally\n",
    "        tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "\n",
    "        # Randomly rotate\n",
    "        tf.keras.layers.RandomRotation(factor=0.12),\n",
    "\n",
    "        # Random zoom\n",
    "        tf.keras.layers.RandomZoom(height_factor=(-0.35, 0.35), # Corresponds to [0.8, 1.2] of original height\n",
    "                                   width_factor=(-0.35, 0.35)), # Corresponds to [0.8, 1.2] of original width\n",
    "\n",
    "        # Random shift\n",
    "        tf.keras.layers.RandomTranslation(height_factor=0.20,\n",
    "                                          width_factor=0.20),\n",
    "\n",
    "        # Contrast\n",
    "        tf.keras.layers.RandomContrast(factor=0.25),\n",
    "\n",
    "    ],\n",
    "    name=\"geometric_augmentations\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define some stuff\n",
    "num_classes = arthropoda_train['family'].nunique() #number of classes = number of families\n",
    "batch_size = 64\n",
    "input_shape = (224, 224, 3)\n",
    "image_size = (224, 224)\n",
    "value_range = (0.0, 1.0)\n",
    "num_classes = 17  \n",
    "\n",
    "# Define callbacks\n",
    "my_callbacks = [\n",
    "callbacks.EarlyStopping(patience=10, restore_best_weights=True),\n",
    "callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5),\n",
    "callbacks.ModelCheckpoint(\"best_model.keras\", save_best_only=True)\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 589\n",
      "Validation size: 148\n",
      "Test size: 185\n"
     ]
    }
   ],
   "source": [
    "# Define root directory\n",
    "root_dir = \"/home/sacar/DeepLearning2425/rare_species\"\n",
    "\n",
    "# Construct full image paths\n",
    "arthropoda_train['full_path'] = arthropoda_train['file_path'].apply(lambda x: os.path.normpath(os.path.join(root_dir, x)))\n",
    "arthropoda_test['full_path'] = arthropoda_test['file_path'].apply(lambda x: os.path.normpath(os.path.join(root_dir, x)))\n",
    "\n",
    "# Extract file paths and labels\n",
    "file_paths_train = arthropoda_train['full_path'].tolist()\n",
    "labels_train = arthropoda_train['family'].tolist()\n",
    "\n",
    "file_paths_test = arthropoda_test['full_path'].tolist()\n",
    "labels_test = arthropoda_test['family'].tolist()\n",
    "\n",
    "# Map string labels to integer indices\n",
    "label_names = sorted(set(labels_train))\n",
    "label_to_index = {name: i for i, name in enumerate(label_names)}\n",
    "\n",
    "# Encode labels\n",
    "labels_train = [label_to_index[label] for label in labels_train]\n",
    "labels_test = [label_to_index[label] for label in labels_test]\n",
    "\n",
    "# Convert to numpy arrays\n",
    "labels_train = np.array(labels_train, dtype=np.int32)\n",
    "labels_test = np.array(labels_test, dtype=np.int32)\n",
    "\n",
    "# --- Train/Validation Split ---\n",
    "combined = list(zip(file_paths_train, labels_train))\n",
    "random.seed(42)\n",
    "random.shuffle(combined)\n",
    "\n",
    "file_paths_train, labels_train = zip(*combined)  # Still tuples at this point\n",
    "split_index = int(0.8 * len(file_paths_train))\n",
    "\n",
    "train_paths = list(file_paths_train[:split_index])\n",
    "train_labels = np.array(labels_train[:split_index], dtype=np.int32)\n",
    "val_paths   = list(file_paths_train[split_index:])\n",
    "val_labels  = np.array(labels_train[split_index:], dtype=np.int32)\n",
    "file_paths_test = list(file_paths_test)  # Ensure it's a list\n",
    "labels_test = np.array(labels_test, dtype=np.int32)\n",
    "\n",
    "print(\"Train size:\", len(train_paths))\n",
    "print(\"Validation size:\", len(val_paths))\n",
    "print(\"Test size:\", len(file_paths_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to build dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to build the dataset\n",
    "def build_dataset(file_paths, labels, preprocess_fn, augment=False):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((file_paths, labels))\n",
    "    dataset = dataset.map(lambda x, y: process_image(x, y, preprocess_fn), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.cache()\n",
    "    if augment:\n",
    "        dataset = dataset.map(augment_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        dataset = dataset.shuffle(buffer_size=1000, reshuffle_each_iteration=True, seed=42)\n",
    "    dataset = dataset.batch(8).prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confirm GPU is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Physical devices cannot be modified after being initialized\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ GPU(s) detected: ['/physical_device:GPU:0']\n"
     ]
    }
   ],
   "source": [
    "# Print GPU devices detected\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f\"✅ GPU(s) detected: {[gpu.name for gpu in gpus]}\")\n",
    "else:\n",
    "    print(\"❌ No GPU detected by TensorFlow.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Models and Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_random_search(train, val, num_classes, base_model_class, n_trials=10, seed=42):\n",
    "    import random, itertools, gc\n",
    "    from tensorflow.keras import layers, models, backend as K\n",
    "    from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "    import pandas as pd\n",
    "    import tensorflow.keras as keras\n",
    "\n",
    "    random.seed(seed)\n",
    "\n",
    "    # Hyperparameter search space\n",
    "    dropout_rates = [0.5, 0.6, 0.7]\n",
    "    dense_units_list = [64, 128]\n",
    "    learning_rates = [1e-5, 5e-5, 1e-4]\n",
    "    patience_values = [5, 7]\n",
    "    freeze_until_layers = [100, 120, 140]\n",
    "    optimizers_list = ['adam']\n",
    "\n",
    "    # Generate all possible combinations\n",
    "    all_combinations = list(itertools.product(\n",
    "        dropout_rates,\n",
    "        dense_units_list,\n",
    "        learning_rates,\n",
    "        patience_values,\n",
    "        freeze_until_layers,\n",
    "        optimizers_list\n",
    "    ))\n",
    "\n",
    "    sampled_combinations = random.sample(all_combinations, k=min(n_trials, len(all_combinations)))\n",
    "    results = []\n",
    "\n",
    "    # ✅ Load weights only once\n",
    "    base_model_template = base_model_class(\n",
    "        input_shape=(224, 224, 3),\n",
    "        include_top=False,\n",
    "        weights='imagenet'\n",
    "    )\n",
    "    base_weights = base_model_template.get_weights()\n",
    "    del base_model_template\n",
    "\n",
    "    for i, (dropout, units, lr, patience, freeze_until, opt_name) in enumerate(sampled_combinations):\n",
    "        print(f\"\\n Trial {i+1}/{len(sampled_combinations)}\")\n",
    "        print(f\"Dropout={dropout}, Units={units}, LR={lr}, Patience={patience}, FreezeUntil={freeze_until}\")\n",
    "\n",
    "        base_model = base_model_class(\n",
    "            input_shape=(224, 224, 3),\n",
    "            include_top=False,\n",
    "            weights=None  # important!\n",
    "        )\n",
    "        base_model.set_weights(base_weights)\n",
    "        base_model.trainable = False\n",
    "\n",
    "        def create_optimizer():\n",
    "            return keras.optimizers.Adam(learning_rate=lr)\n",
    "\n",
    "        model = models.Sequential([\n",
    "            base_model,\n",
    "            layers.GlobalAveragePooling2D(),\n",
    "            layers.Dropout(dropout),\n",
    "            layers.Dense(units, activation='relu'),\n",
    "            layers.Dense(num_classes, activation='softmax')\n",
    "        ])\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=create_optimizer(),\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "\n",
    "        callbacks_list = [\n",
    "            EarlyStopping(patience=patience, restore_best_weights=True),\n",
    "            ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=max(1, patience // 2)),\n",
    "            ModelCheckpoint(f\"{base_model_class.__name__}_V2_clahe{i+1}.keras\", save_best_only=True)\n",
    "        ]\n",
    "\n",
    "        # Phase 1 training\n",
    "        history1 = model.fit(\n",
    "            train,\n",
    "            validation_data=val,\n",
    "            epochs=30,\n",
    "            callbacks=callbacks_list,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        # Phase 2 fine-tuning\n",
    "        base_model.trainable = True\n",
    "        for layer in base_model.layers[:freeze_until]:\n",
    "            layer.trainable = False\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=create_optimizer(),\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "\n",
    "        history2 = model.fit(\n",
    "            train,\n",
    "            validation_data=val,\n",
    "            epochs=30,\n",
    "            callbacks=callbacks_list,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        final_val_acc = history2.history['val_accuracy'][-1]\n",
    "\n",
    "        results.append({\n",
    "            'dropout': dropout,\n",
    "            'dense_units': units,\n",
    "            'learning_rate': lr,\n",
    "            'patience': patience,\n",
    "            'freeze_until': freeze_until,\n",
    "            'optimizer': opt_name,\n",
    "            'val_accuracy': final_val_acc\n",
    "        })\n",
    "\n",
    "        # Cleanup\n",
    "        del model, base_model, history1, history2, callbacks_list\n",
    "        K.clear_session()\n",
    "        gc.collect()\n",
    "\n",
    "    # ✅ RETURN after the loop ends\n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_random_search_results(results_df, top_n=5):\n",
    "    sns.set(style=\"whitegrid\")\n",
    "\n",
    "    # -------------------------------\n",
    "    # 1. Top N Configurations by Val Accuracy\n",
    "    # -------------------------------\n",
    "    top_configs = results_df.sort_values(by='val_accuracy', ascending=False).head(top_n)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(data=top_configs, x='val_accuracy', y=top_configs.index, hue='dropout')\n",
    "    plt.title(f\"Top {top_n} Hyperparameter Configs by Validation Accuracy\")\n",
    "    plt.xlabel(\"Validation Accuracy\")\n",
    "    plt.ylabel(\"Config Index\")\n",
    "    plt.legend(title=\"Dropout\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # -------------------------------\n",
    "    # 2. Strip plots: Accuracy vs Each Hyperparam\n",
    "    # -------------------------------\n",
    "    fig, axs = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    fig.suptitle(\"Validation Accuracy vs Hyperparameters\", fontsize=16)\n",
    "\n",
    "    sns.stripplot(data=results_df, x='dropout', y='val_accuracy', ax=axs[0, 0])\n",
    "    axs[0, 0].set_title(\"Dropout\")\n",
    "\n",
    "    sns.stripplot(data=results_df, x='dense_units', y='val_accuracy', ax=axs[0, 1])\n",
    "    axs[0, 1].set_title(\"Dense Units\")\n",
    "\n",
    "    sns.stripplot(data=results_df, x='learning_rate', y='val_accuracy', ax=axs[0, 2])\n",
    "    axs[0, 2].set_title(\"Learning Rate\")\n",
    "\n",
    "    sns.stripplot(data=results_df, x='patience', y='val_accuracy', ax=axs[1, 0])\n",
    "    axs[1, 0].set_title(\"EarlyStopping Patience\")\n",
    "\n",
    "    sns.stripplot(data=results_df, x='freeze_until', y='val_accuracy', ax=axs[1, 1])\n",
    "    axs[1, 1].set_title(\"Freeze Until Layer\")\n",
    "\n",
    "    axs[1, 2].axis('off')  # Empty slot\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "    # -------------------------------\n",
    "    # 3. Heatmap (e.g. Dropout vs Units)\n",
    "    # -------------------------------\n",
    "    pivot_table = results_df.pivot_table(\n",
    "        values='val_accuracy',\n",
    "        index='dropout',\n",
    "        columns='dense_units',\n",
    "        aggfunc='mean'\n",
    "    )\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(pivot_table, annot=True, fmt=\".3f\", cmap=\"viridis\")\n",
    "    plt.title(\"Heatmap: Dropout vs Dense Units (Val Accuracy)\")\n",
    "    plt.xlabel(\"Dense Units\")\n",
    "    plt.ylabel(\"Dropout Rate\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Running model: MobileNetV2\n",
      "\n",
      "\n",
      " Trial 1/10\n",
      "Dropout=0.7, Units=64, LR=5e-05, Patience=7, FreezeUntil=100\n",
      "Epoch 1/30\n"
     ]
    }
   ],
   "source": [
    "#models_list = ['inceptionv3', 'MobileNetV2', 'efficientnetb0', 'densenet121', 'resnet50']\n",
    "models_list = ['MobileNetV2']\n",
    "\n",
    "for model_name in models_list:\n",
    "    print(f\"\\n>>> Running model: {model_name}\\n\")\n",
    "\n",
    "    # Get model constructor and preprocessing function from your model map\n",
    "    base_model_class, preprocess_fn = model_map[model_name]\n",
    "\n",
    "    # ✅ Build datasets using clean inputs\n",
    "    train = build_dataset(train_paths, train_labels, preprocess_fn, augment=True)\n",
    "    val   = build_dataset(val_paths, val_labels, preprocess_fn, augment=False)\n",
    "    test  = build_dataset(file_paths_test, labels_test, preprocess_fn, augment=False)\n",
    "\n",
    "    # Run hyperparameter search\n",
    "    results = run_random_search(\n",
    "        train, val, num_classes,\n",
    "        base_model_class=base_model_class,\n",
    "        n_trials=10,\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    # Plot results\n",
    "    plot_random_search_results(results, top_n=1)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_wsl_env)",
   "language": "python",
   "name": "tf_wsl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
